# Chapter 4 -  Clustering and classification


```{r}
date()
```

In this chapter I study the Boston dataset from MASS library. The dataset contains info about housing Values in Suburbs of Boston. At first, I load all the necessary libraries, and the data and check its basic structure

## Loading and descripbing the dataset

```{r}
library(MASS)
library(dplyr)
library(GGally)
library(corrplot)
data("Boston")
dim(Boston)
str(Boston)
```

The dataset contains 14 variables and 506 observations. Here are descriptions for each variable from https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html

crim: per capita crime rate by town.
zn: proportion of residential land zoned for lots over 25,000 sq.ft.
indus: proportion of non-retail business acres per town.
chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
nox: nitrogen oxides concentration (parts per 10 million).
rm: average number of rooms per dwelling.
age: proportion of owner-occupied units built prior to 1940.
dis: weighted mean of distances to five Boston employment centres.
rad: index of accessibility to rasdial highways.
tax: full-value property-tax rate per \$10,000.
ptratio: pupil-teacher ratio by town.
black: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
lstat: lower status of the population (percent).
medv: median value of owner-occupied homes in \$1000s.

Next, let's look at the summaries of the variables:

```{r}

summary(Boston)

for (col in seq(from=1, ncol(Boston), by=3)) {
  par(mfrow = c(1,3))
  hist(Boston[,col], xlab = colnames(Boston)[col], main = colnames(Boston)[col] )
  hist(Boston[,col+1], xlab = colnames(Boston)[col+1], main = colnames(Boston)[col+1])
  if (col+2<= ncol(Boston)) 
    hist(Boston[,col+2], xlab = colnames(Boston)[col+2], main = colnames(Boston)[col+2])
}


# Looking at variable correlations on scaled data
cor_matrix<-cor(Boston) %>% round(digits = 2)

# visualizing the correlation matrix
corrplot(cor_matrix, method="color", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)

```

Most of the variables don't seem to follow normal distribution. 

For example crim (crime rate), zn adn dis are heavily skewed to the left, whereas age, black and ptratio are heavily skewed to right,  

The correlation plot shows highest position correlation between: 
- rad and tax (highway accessiblility and taxes)
- nox and indus (nitrogen oxide levels and non-rental properties)
- nm and medv (average number of rooms per dwelling and property value)

And the highest negative correlatiosn betwee 

- lstat and medv (lower population status and value)
- age and dis (property age and distance to Boston's employment centres)
- nox and dis (nitrogen oxide levels and and distance to Boston's employment centres)
- indus and dis (non-rental properties and distance to Boston's employment centres)

```{r}
(p <- ggpairs(Boston, mapping = aes( alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20))))
```

## Standardizing the dataset

Here I scale the whole dataset to mean of 1

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)


2# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# summaries of the scaled variables
summary(boston_scaled)


```



```{r}

for (col in seq(from=1, ncol(boston_scaled), by=3)) {
  par(mfrow = c(1,3))
  hist(boston_scaled[,col], xlab = colnames(boston_scaled)[col], main = colnames(boston_scaled)[col] )
  hist(boston_scaled[,col+1], xlab = colnames(boston_scaled)[col+1], main = colnames(boston_scaled)[col+1])
  if (col+2<= ncol(boston_scaled)) 
    hist(boston_scaled[,col+2], xlab = colnames(boston_scaled)[col+2], main = colnames(boston_scaled)[col+2])
}

```

The scaled variables have mean of 1, however the histograms show that simple scaling doesn't normalize the distributions. 

## Categorizing crime rate, and dividing dataset to train and test sets

Next, make a new 'crime' variable by categorizing the 'crim', and remove the continuous 'crim' from the dataset


```{r}

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, labels = c("low", "med_low", "med_high", "high"),  include.lowest = TRUE)


# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)


```


```{r}
# choose randomly 80% of the rows
ind <- sample(nrow(boston_scaled),  size = nrow(boston_scaled) * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]
```

## Linear discriminant analysis

Running linear discriminant analysis (LDA) and plotting results. LDA is a classification method, whihc aims to to find a linear combination of features that characterizes or separates two or more classes of objects or events. 

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)


```


Next, saving the correct 'crime' classes in another variable, and removing the 'crime' column from the test set.Then predicting the crime classes with ldafit model using test data

```{r}
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```

The cross table shows 63 of 102 predictions were correct, giving model about 62% accuracy. The hardest classes to predict were med_low and med_high classess, in which threre were lots of errors.


Finally, I'll look at the distances in the Boston data

```{r}
# Loading the original unscaled Boston data again
data('Boston')

# Scale the variables
boston_scaled<-scale(Boston)

# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(Boston,  method = "manhattan")

# look at the summary of the distances
summary(dist_man)

```

Here I look at the data with K-means method, using 3 clusters 

```{r}
km <-kmeans(Boston, centers = 3)

# plot the Boston dataset with clusters
pairs(Boston[6:10], col = km$cluster)
```

here I'll look at the data with different amount of clusters to find out optimal number in explaining the data

```{r}
# MASS, ggplot2 and Boston dataset are available
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
```
Total of within cluster sum of squares (WCSS) gives suggestion for optimal number of clusters. The plot shows that WCSS decreases a lot with just 2 cluseters, suggesting that just 2 centers sufficiently cluster the data. 

